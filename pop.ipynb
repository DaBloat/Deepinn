{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, adjacent=None | list,\n",
    "                       weights=None | list,\n",
    "                       activation='sigmoid',\n",
    "                       name=None | str,\n",
    "                       value=0.0):\n",
    "        self.name=name\n",
    "        self.adjacent = adjacent\n",
    "        self.weights = weights\n",
    "        self.activation = activation\n",
    "        self.inputs = None\n",
    "        self.value = value\n",
    "\n",
    "    def process_output(self):\n",
    "        print(f'Process in {self.name}')\n",
    "        self.get_adjacent_vals()\n",
    "        print(f'Adjacent: {self.inputs}')\n",
    "        self.summation_func()\n",
    "        print(f'Summation: {self.value}')\n",
    "        self.output = self.activation_func(self.activation)\n",
    "        print(f'Activation Function - {self.activation}: {self.output}')\n",
    "        print()\n",
    "        \n",
    "    def set_name(self, name = None | str):\n",
    "        self.name = name\n",
    "        \n",
    "    def set_adjacent(self, adjacent = None | list):\n",
    "        self.adjacent = adjacent\n",
    "\n",
    "    def get_adjacent_vals(self):\n",
    "        self.inputs = [i.get_val() for i in self.adjacent]\n",
    "        return self.inputs\n",
    "\n",
    "    def get_adjacent_names(self):\n",
    "        return [i.name for i in self.adjacent]\n",
    "\n",
    "    def summation_func(self):\n",
    "        self.value = round(np.dot(self.inputs, self.weights), 5)\n",
    "        return self.value\n",
    "\n",
    "    def activation_func(self, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            self.value = self.sigmoid(self.value)\n",
    "        elif activation == 'relu':\n",
    "            self.value = self.relu(self.value)\n",
    "        elif activation == 'softmax':\n",
    "            self.value = self.value\n",
    "        return self.value\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return round(1/(1 + np.exp(-x)), 5)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return round(np.maximum(0, x))\n",
    "\n",
    "    def get_val(self):\n",
    "        return self.value\n",
    "    \n",
    "    def set_val(self, val):\n",
    "        self.value = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "  def __init__(self, **kwags):\n",
    "    self.process = kwags\n",
    "    # (input, numOfNodes, values)\n",
    "    # (hidden, numOfNodes, weights, bias T or F)\n",
    "    # (end, numOfNodes, weights, bias T or F)\n",
    "    # (bias, numOfNodes, values)\n",
    "    self.inputs = []\n",
    "    self.hidden = []\n",
    "    self.output = []\n",
    "    self.bias = []\n",
    "\n",
    "  def set_bias(self):\n",
    "    self.bias = [Neuron(name=f\"BIAS {node}\", value=self.process['bias'][1][node]) for node in range(self.process['bias'][0])]\n",
    "\n",
    "  def set_input(self):\n",
    "    self.inputs = [Neuron(name=f\"INPUT {node}\", value=self.process['input'][1][node]) for node in range(self.process['input'][0])]\n",
    "\n",
    "  def set_hidden(self):\n",
    "    self.hidden = [Neuron(name=f\"HIDDEN {node}\", weights=self.process['hidden'][1][node], adjacent=self.inputs + [self.bias[0]]) for node in range(self.process['hidden'][0])]\n",
    "    for nodes in self.hidden:\n",
    "      nodes.process_output()\n",
    "\n",
    "  def set_output(self):\n",
    "    self.output = [Neuron(name=f\"OUTPUT {node}\", weights=self.process['output'][1][node], adjacent=self.hidden + [self.bias[1]]) for node in range(self.process['output'][0])]\n",
    "    for nodes in self.output:\n",
    "      nodes.process_output()\n",
    "\n",
    "  def compile(self):\n",
    "    self.set_bias()\n",
    "    self.set_input()\n",
    "    self.set_hidden()\n",
    "    self.set_output()\n",
    "\n",
    "  def show_output(self):\n",
    "    for nodes in self.output:\n",
    "      print(f\"{nodes.name} : {nodes.get_val()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Depth():\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN():\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process in H1\n",
      "Adjacent: [0.05, 0.1, 1]\n",
      "Summation: 0.3775\n",
      "Activation Function - sigmoid: 0.59327\n",
      "\n",
      "Process in H2\n",
      "Adjacent: [0.05, 0.1, 1]\n",
      "Summation: 0.3925\n",
      "Activation Function - sigmoid: 0.59688\n",
      "\n",
      "Process in O1\n",
      "Adjacent: [np.float64(0.59327), np.float64(0.59688), 1]\n",
      "Summation: 1.1059\n",
      "Activation Function - sigmoid: 0.75136\n",
      "\n",
      "Process in O2\n",
      "Adjacent: [np.float64(0.59327), np.float64(0.59688), 1]\n",
      "Summation: 1.22492\n",
      "Activation Function - sigmoid: 0.77293\n",
      "\n",
      "output1 = 0.75136\n",
      "output2 = 0.77293\n"
     ]
    }
   ],
   "source": [
    "# Structure\n",
    "i1 = Neuron(name='I1', value=.05)\n",
    "i2 = Neuron(name='I2', value=.1)\n",
    "b1 = Neuron(name='B1', value=1)\n",
    "\n",
    "h1 = Neuron(name='H1', adjacent=[i1, i2, b1], weights=[.15, .20, .35])\n",
    "h2 = Neuron(name='H2', adjacent=[i1, i2, b1], weights=[.25, .30, .35])\n",
    "\n",
    "b2 = Neuron(name='B2', value=1)\n",
    "\n",
    "o1 = Neuron(name='O1', adjacent=[h1, h2, b2], weights=[.40, .45, .60])\n",
    "o2 = Neuron(name='O2', adjacent=[h1, h2, b2], weights=[.50, .55, .60])\n",
    "\n",
    "h1.process_output()\n",
    "h2.process_output()\n",
    "o1.process_output()\n",
    "o2.process_output()\n",
    "print(f'output1 = {o1.get_val()}')\n",
    "print(f'output2 = {o2.get_val()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = 0.98\n",
    "b2 = 0.92\n",
    "w1 = [0.87, 0.93] + [b1]\n",
    "w2 = [0.59, 0.69] + [b1]\n",
    "w3 = [0.89, 0.89] + [b2]\n",
    "w4 = [0.81, 0.95] + [b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process in HIDDEN 0\n",
      "Adjacent: [0.05, 0.1, 1]\n",
      "Summation: 1.1165\n",
      "Activation Function - sigmoid: 0.75334\n",
      "\n",
      "Process in HIDDEN 1\n",
      "Adjacent: [0.05, 0.1, 1]\n",
      "Summation: 1.0785\n",
      "Activation Function - sigmoid: 0.74621\n",
      "\n",
      "Process in OUTPUT 0\n",
      "Adjacent: [np.float64(0.75334), np.float64(0.74621), 1]\n",
      "Summation: 2.2546\n",
      "Activation Function - sigmoid: 0.90505\n",
      "\n",
      "Process in OUTPUT 1\n",
      "Adjacent: [np.float64(0.75334), np.float64(0.74621), 1]\n",
      "Summation: 2.2391\n",
      "Activation Function - sigmoid: 0.90371\n",
      "\n",
      "OUTPUT 0 : 0.90505\n",
      "OUTPUT 1 : 0.90371\n"
     ]
    }
   ],
   "source": [
    "p = FeedForwardNN(\n",
    "               input=(2, (0.05, 0.10)),\n",
    "               hidden=(2, (w1, w2)),\n",
    "               output=(2, (w3, w4)),\n",
    "               bias=(2, (1, 1))\n",
    ")\n",
    "p.compile()\n",
    "p.show_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning with depth\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self,\n",
    "                 name = None | list,\n",
    "                 bias = None | list,\n",
    "                 values = None | list,\n",
    "                 weights = None,\n",
    "                 act_func = 'sigmoid',\n",
    "                 **kwargs):\n",
    "        self.name = name\n",
    "        self.bias = bias\n",
    "        self.values = values\n",
    "        self.act_func = act_func\n",
    "        self.weights = weights\n",
    "        self.layer = []\n",
    "        self.process_init()\n",
    "        \n",
    "    def process_init(self):\n",
    "        if self.weights is None:\n",
    "            self.layer = [Neuron(name=self.name + str(node), \n",
    "                                 value=self.values[node], \n",
    "                                 activation=self.act_func) for node in range(len(self.values))] + [Neuron(name=self.name + 'bias', value=1, activation=self.act_func)]\n",
    "        else:\n",
    "            self.layer = [Neuron(name=self.name + str(node), weights=self.weights[node] + [self.bias[node]], \n",
    "                                 activation=self.act_func) for node in range(len(self.weights))] \n",
    "        \n",
    "    def get_all_values(self):\n",
    "        return [node.value for node in self.layer]\n",
    "    \n",
    "    def get_all_weights(self):\n",
    "        return [node.weights for node in self.layer]\n",
    "    \n",
    "    def get_all_names(self):\n",
    "        return [node.name for node in self.layer]\n",
    "    \n",
    "    def get_all_adjacencies(self):\n",
    "        return [node.get_adjacent_names() for node in self.layer]\n",
    "        \n",
    "    def set_adjacencies(self, adj):\n",
    "        for node in self.layer:\n",
    "            node.set_adjacent(adj)\n",
    "    \n",
    "    def compile(self):\n",
    "        for node in self.layer:\n",
    "            node.process_output()\n",
    "        if self.act_func == 'softmax':\n",
    "            print(f'FINAL LAYER')\n",
    "            summation = np.sum([np.exp(node.value) for node in self.layer])\n",
    "            print([round(np.exp(node.value) / summation, 2) for node in self.layer])\n",
    "        else:\n",
    "            print(f' output={[node.value for node in self.layer]}')\n",
    "            self.layer.append(Neuron(name=self.name + 'bias', value=1))\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 1, 1]\n",
      "['input0', 'input1', 'input2', 'input3', 'inputbias']\n"
     ]
    }
   ],
   "source": [
    "input = Layer(name='input', values=[2,1,2,1], bias = [1])\n",
    "l1 = Layer(name='layer1-', weights=[[1.00, 0.00, -1.00, 1.00], [0.00, 1.00, 1.00, 0.00], [1.00, -1.00, 0.00, 0.00]], bias=[1, 1, 0], act_func='relu')\n",
    "l1.set_adjacencies(input.layer)\n",
    "\n",
    "print(input.get_all_values())\n",
    "print(input.get_all_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.0, -1.0, 1.0, 1], [0.0, 1.0, 1.0, 0.0, 1], [1.0, -1.0, 0.0, 0.0, 0]]\n",
      "['layer1-0', 'layer1-1', 'layer1-2']\n",
      "[['input0', 'input1', 'input2', 'input3', 'inputbias'], ['input0', 'input1', 'input2', 'input3', 'inputbias'], ['input0', 'input1', 'input2', 'input3', 'inputbias']]\n",
      "\n",
      "Process in layer1-0\n",
      "Adjacent: [2, 1, 2, 1, 1]\n",
      "Summation: 2.0\n",
      "Activation Function - relu: 2\n",
      "\n",
      "Process in layer1-1\n",
      "Adjacent: [2, 1, 2, 1, 1]\n",
      "Summation: 4.0\n",
      "Activation Function - relu: 4\n",
      "\n",
      "Process in layer1-2\n",
      "Adjacent: [2, 1, 2, 1, 1]\n",
      "Summation: 1.0\n",
      "Activation Function - relu: 1\n",
      "\n",
      " output=[2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "print(l1.get_all_weights())\n",
    "print(l1.get_all_names())\n",
    "print(l1.get_all_adjacencies())\n",
    "print()\n",
    "l1.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = Layer(name='layer2-', weights=[[0.00, -1.00, 2.00], [-1.00, 1.00, 0.00], [0.00, 1.00, 1.00], [-1.00, 0.00, 0.00]], bias=[1, 2, 1, 3], act_func='relu')\n",
    "l2.set_adjacencies(l1.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, -1.0, 2.0, 1], [-1.0, 1.0, 0.0, 2], [0.0, 1.0, 1.0, 1], [-1.0, 0.0, 0.0, 3]]\n",
      "['layer2-0', 'layer2-1', 'layer2-2', 'layer2-3']\n",
      "[['layer1-0', 'layer1-1', 'layer1-2', 'layer1-bias'], ['layer1-0', 'layer1-1', 'layer1-2', 'layer1-bias'], ['layer1-0', 'layer1-1', 'layer1-2', 'layer1-bias'], ['layer1-0', 'layer1-1', 'layer1-2', 'layer1-bias']]\n",
      "\n",
      "Process in layer2-0\n",
      "Adjacent: [2, 4, 1, 1]\n",
      "Summation: -1.0\n",
      "Activation Function - relu: 0\n",
      "\n",
      "Process in layer2-1\n",
      "Adjacent: [2, 4, 1, 1]\n",
      "Summation: 4.0\n",
      "Activation Function - relu: 4\n",
      "\n",
      "Process in layer2-2\n",
      "Adjacent: [2, 4, 1, 1]\n",
      "Summation: 6.0\n",
      "Activation Function - relu: 6\n",
      "\n",
      "Process in layer2-3\n",
      "Adjacent: [2, 4, 1, 1]\n",
      "Summation: 1.0\n",
      "Activation Function - relu: 1\n",
      "\n",
      " output=[0, 4, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "print(l2.get_all_weights())\n",
    "print(l2.get_all_names())\n",
    "print(l2.get_all_adjacencies())\n",
    "print()\n",
    "l2.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Layer(name='out-', weights=[[0.00, -1.00, -1.00, 2.00], [-1.00, 0.00, 0.00, 0.00], [-1.00, 0.00, 1.00, 0.00]], bias=[5, 2, -4], act_func='softmax')\n",
    "out.set_adjacencies(l2.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, -1.0, -1.0, 2.0, 5], [-1.0, 0.0, 0.0, 0.0, 2], [-1.0, 0.0, 1.0, 0.0, -4]]\n",
      "['out-0', 'out-1', 'out-2']\n",
      "[['layer2-0', 'layer2-1', 'layer2-2', 'layer2-3', 'layer2-bias'], ['layer2-0', 'layer2-1', 'layer2-2', 'layer2-3', 'layer2-bias'], ['layer2-0', 'layer2-1', 'layer2-2', 'layer2-3', 'layer2-bias']]\n",
      "\n",
      "Process in out-0\n",
      "Adjacent: [0, 4, 6, 1, 1]\n",
      "Summation: -3.0\n",
      "Activation Function - softmax: -3.0\n",
      "\n",
      "Process in out-1\n",
      "Adjacent: [0, 4, 6, 1, 1]\n",
      "Summation: 2.0\n",
      "Activation Function - softmax: 2.0\n",
      "\n",
      "Process in out-2\n",
      "Adjacent: [0, 4, 6, 1, 1]\n",
      "Summation: 2.0\n",
      "Activation Function - softmax: 2.0\n",
      "\n",
      "FINAL LAYER\n",
      "[np.float64(0.0), np.float64(0.5), np.float64(0.5)]\n"
     ]
    }
   ],
   "source": [
    "print(out.get_all_weights())\n",
    "print(out.get_all_names())\n",
    "print(out.get_all_adjacencies())\n",
    "print()\n",
    "out.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepANN():\n",
    "    def __init__(self, *args):\n",
    "        lst = [list(args).pop(0)]\n",
    "        self.compiled = []\n",
    "        for layer in args:\n",
    "            layer.set_adjacencies(lst[-1].layer)\n",
    "            print(layer.get_all_adjacencies())\n",
    "            lst.append(layer)\n",
    "            self.compiled.append(layer)\n",
    "            \n",
    "    def compile(self):\n",
    "        for layer in self.compiled:\n",
    "            layer.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['input0', 'input1', 'input2', 'input3', 'inputbias'], ['input0', 'input1', 'input2', 'input3', 'inputbias'], ['input0', 'input1', 'input2', 'input3', 'inputbias'], ['input0', 'input1', 'input2', 'input3', 'inputbias'], ['input0', 'input1', 'input2', 'input3', 'inputbias']]\n",
      "[['input0', 'input1', 'input2', 'input3', 'inputbias'], ['input0', 'input1', 'input2', 'input3', 'inputbias'], ['input0', 'input1', 'input2', 'input3', 'inputbias']]\n",
      "[['layer1-0', 'layer1-1', 'layer1-2'], ['layer1-0', 'layer1-1', 'layer1-2'], ['layer1-0', 'layer1-1', 'layer1-2'], ['layer1-0', 'layer1-1', 'layer1-2']]\n",
      "[['layer2-0', 'layer2-1', 'layer2-2', 'layer2-3'], ['layer2-0', 'layer2-1', 'layer2-2', 'layer2-3'], ['layer2-0', 'layer2-1', 'layer2-2', 'layer2-3']]\n"
     ]
    }
   ],
   "source": [
    "model = DeepANN(\n",
    "    Layer(name='input', values=[2,1,2,1], bias = [1]),\n",
    "    Layer(name='layer1-', weights=[[1.00, 0.00, -1.00, 1.00], [0.00, 1.00, 1.00, 0.00], [1.00, -1.00, 0.00, 0.00]], bias=[1, 1, 0], act_func='relu'),\n",
    "    Layer(name='layer2-', weights=[[0.00, -1.00, 2.00], [-1.00, 1.00, 0.00], [0.00, 1.00, 1.00], [-1.00, 0.00, 0.00]], bias=[1, 2, 1, 3], act_func='relu'),\n",
    "    Layer(name='out-', weights=[[0.00, -1.00, -1.00, 2.00], [-1.00, 0.00, 0.00, 0.00], [-1.00, 0.00, 1.00, 0.00]], bias=[5, 2, -4], act_func='softmax')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process in input0\n",
      "Adjacent: [2, 1, 2, 1, 1]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'types.UnionType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[184], line 13\u001b[0m, in \u001b[0;36mDeepANN.compile\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled:\n\u001b[0;32m---> 13\u001b[0m         \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[151], line 46\u001b[0m, in \u001b[0;36mLayer.compile\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m---> 46\u001b[0m         \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_func \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFINAL LAYER\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[137], line 18\u001b[0m, in \u001b[0;36mNeuron.process_output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_adjacent_vals()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdjacent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummation_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation)\n",
      "Cell \u001b[0;32mIn[137], line 38\u001b[0m, in \u001b[0;36mNeuron.summation_func\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msummation_func\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'types.UnionType'"
     ]
    }
   ],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensor",
   "language": "python",
   "name": ".pyenvtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
